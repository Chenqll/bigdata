{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQsBjvDmR+jUeo5WXCq2Bp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"em4PXsoPQPpW"},"source":["# Workshop5: Demonstrate a simple (DL) Recommendation Architecture based on learning user and item embeddings and then combining with a dot product "]},{"cell_type":"code","metadata":{"id":"hTxwX4rSo9ai"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8upCLXwPvxWR","executionInfo":{"status":"ok","timestamp":1673606887307,"user_tz":-480,"elapsed":10134,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error\n","import tensorflow.keras as ks\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TB9GPAVxWn3"},"source":["#%cd /content/drive/MyDrive/recsys\n","#from demolib import mapdata"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZHsXqRdwRRH"},"source":["# reload the movielens dataset\n","file = \"/content/drive/My Drive/recsys/u_data.csv\"\n","ratings_df = pd.read_csv(file)\n","ratings_df.columns = ['user_id','item_id','rating','datetime']\n","ratings_df.drop('datetime',axis=1,inplace=True)\n","ratings_df['item_id'] = ratings_df['item_id'].astype(str)\n","print(ratings_df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we re-use some code from workshop2 to map the user_id's and item_id's in the raw ratings data to integer indexes\n","def mapdata(ratings_df):\n","  ratings_df[\"item_id\"] = ratings_df[\"item_id\"].astype(str)\n","  ratings_df[\"user_id\"] = ratings_df[\"user_id\"].astype(str)\n","  ratings_df[\"rating\"]  = ratings_df[\"rating\"].values.astype(np.float32)\n","  user_ids = np.sort(ratings_df[\"user_id\"].unique()).tolist()\n","  umap = {x: i for i, x in enumerate(user_ids)}\n","  item_ids = np.sort(ratings_df[\"item_id\"].unique()).tolist()\n","  imap = {x: i for i, x in enumerate(item_ids)}\n","  ratings_df[\"user_id\"] = ratings_df[\"user_id\"].map(umap) # swap userid for user index\n","  ratings_df[\"item_id\"] = ratings_df[\"item_id\"].map(imap) # swap itemid for item index\n","  return ratings_df, umap, imap"],"metadata":{"id":"1VdnJw6nxjRk","executionInfo":{"status":"ok","timestamp":1673607404576,"user_tz":-480,"elapsed":307,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgTkxJ2BxLQ4","executionInfo":{"status":"ok","timestamp":1673607411237,"user_tz":-480,"elapsed":4,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["ratings_df, umap, imap = mapdata(ratings_df)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcEWQXrSwpSP","executionInfo":{"status":"ok","timestamp":1673607418592,"user_tz":-480,"elapsed":470,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["# train/test creation\n","from sklearn.model_selection import train_test_split\n","train, test = train_test_split(ratings_df, test_size=0.2, random_state=1)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"oByaRLcfwzyf","executionInfo":{"status":"ok","timestamp":1673607420554,"user_tz":-480,"elapsed":311,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["# define the model\n","def definemodel(nusers, nitems, embeddingsdim):\n","    \n","    #add 1 to the number of items & users because the embedding layers need an \n","    #extra row for items & users that do not appear in the training dataset. \n","    \n","    # create item embedding layers\n","    input_items = ks.layers.Input(shape=[1], name=\"item_input\")\n","    embed_items = ks.layers.Embedding(nitems + 1, embeddingsdim, name=\"item_embedding\")(input_items)\n","    items_flat  = ks.layers.Flatten()(embed_items)\n","    \n","    # create user embedding layers\n","    input_users = ks.layers.Input(shape=[1], name=\"user_input\")\n","    embed_users = ks.layers.Embedding(nusers + 1, embeddingsdim, name=\"user_embedding\")(input_users)\n","    users_flat  = ks.layers.Flatten()(embed_users)\n","    \n","    # predicted rating = dotproduct of user and item embeddings\n","    out  = ks.layers.Dot(name=\"dot-product\", axes=1)([items_flat, users_flat])\n","\n","    model = ks.Model([input_items, input_users], out)   \n","    return model"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQ4ndygcw75_"},"source":["# compile the model  \n","# to list the available metrics see: https://www.tensorflow.org/api_docs/python/tf/keras/metrics  \n","embeddingsdim = 15\n","model = definemodel(len(umap), len(imap), embeddingsdim)\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=[ks.metrics.MeanAbsoluteError()])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYLYGWrGyOr4"},"source":["# train the model\n","# for documentaion on keras models, e.g. to see the model.fit() parameters \n","# see https://www.tensorflow.org/api_docs/python/tf/keras/Model\n","\n","hist = model.fit([train.item_id, train.user_id], train.rating, batch_size=64, epochs=5, verbose=1, \n","                 validation_data=([test.item_id, test.user_id], test.rating))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-stx7bJbyWKv"},"source":["# display the model training history\n","train_loss = hist.history['loss']\n","val_loss = hist.history['val_loss']\n","plt.plot(train_loss, color='r', label='Train Loss')\n","plt.plot(val_loss, color='b', label='Validation Loss')\n","plt.title(\"Train and Validation Loss Curve\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JO-WLfylyXKX"},"source":["# test the model\n","predictions = model.predict([test.item_id, test.user_id])\n","\n","# show a sample of the predictions\n","for i in range(0,5):\n","  print(\"actual=\",test.rating.iloc[i],\"pred=\",predictions[i][0], \"abserr=\",abs(test.rating.iloc[i]-predictions[i][0])) \n","  \n","# compute MAE\n","print(\"MAE=\",mean_absolute_error(test.rating, predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsDcQZdSRege"},"source":["# test the model (alternative method)\n","# compute the metrics defined when the model was compiled\n","model.evaluate([test['item_id'],test['user_id']],test['rating'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15iHSbo1RwQr"},"source":["# to show the metrics that are defined for the model\n","model.metrics_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFBnzVhhyZ_J"},"source":["# FYI: can do repeat tests for more accurate results (optional)\n","# each retraining may generate a (slightly) different model\n","embeddingsdim = 15\n","maes = list()\n","for j in range(5):\n","    ks.backend.clear_session()\n","    model = definemodel(len(umap),len(imap),embeddingsdim)\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","    weights = model.get_weights()\n","    res = 0\n","    reps = 2\n","    for i in range(reps):\n","        model.set_weights(weights)\n","        hist = model.fit([train.item_id, train.user_id], train.rating, batch_size=64, epochs=5, verbose=0) # shuffle=False,\n","        predictions = model.predict([test.item_id, test.user_id])\n","        mae = mean_absolute_error(test.rating, predictions)\n","        #print(i, \"mae=\", mae)\n","        res = res + mae\n","    print(j, \"avg mae=\",res/reps)\n","    maes.append(res/reps)  \n","plt.plot(maes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DldWP8NwylTH"},"source":["# Visualizing the Item Embeddings with TensorFlow Embedding Projector"]},{"cell_type":"code","metadata":{"id":"0zM6M8O32A7i","executionInfo":{"status":"ok","timestamp":1673607122907,"user_tz":-480,"elapsed":876,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["# load the movie names from file and create a lookup dict. between item index and title\n","file = \"/content/drive/My Drive/recsys/u_item.csv\"\n","titles = pd.read_csv(file, dtype=str)\n","titles[\"itemidx\"] = titles[\"movie id\"].map(imap)\n","titlelookup = dict(zip(titles[\"itemidx\"],titles[\"movie name\"]))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYv5oqXl2MeO"},"source":["# Extract the item embeddings from the trained model\n","item_em = model.get_layer('item_embedding')\n","item_em_weights = item_em.get_weights()[0]\n","item_em_weights.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_c2aGL52QpW","executionInfo":{"status":"ok","timestamp":1673607127375,"user_tz":-480,"elapsed":754,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["#save two tsv files: one containing the embedding weights, and the other containing the corresponding item title.\n","out_v = open('vecs.tsv', 'w')\n","out_m = open('meta.tsv', 'w',encoding='utf-8')\n","iids =list(ratings_df.item_id.unique())\n","for i in iids:\n","    title = titlelookup[i]\n","    embeddings = item_em_weights[i]\n","    out_m.write(title + \"\\n\")\n","    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n","out_v.close()\n","out_m.close()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zmhlsRM5pZU"},"source":["#check to see that they are now stored in your google drive\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tEw9--R4_ts"},"source":["Now download the two files from Google Drive to your computer\n","\n","Then go to the TensorFlow Embedding Projector page, wait for the default \n","embedding to load, and then click Load to upload your tsv files.\n","\n","http://projector.tensorflow.org/\n"]},{"cell_type":"markdown","metadata":{"id":"RZeH-RAU-2EL"},"source":["# Making recommendations for a given user"]},{"cell_type":"code","metadata":{"id":"qdNU1Ye9-jC0"},"source":["#select a target user and make ratings predictions for all items\n","targetuser = 100\n","user = np.array([targetuser for i in range(len(iids))]) # get a duplicate userid for every possible item\n","pred = model.predict([np.array(iids), user])\n","pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pE-KrnPH-9Oj"},"source":["#sort the rating predictions, and retrieve the index of the highest 5. \n","pred = pred.reshape(-1) #reshape to single dimension\n","preditem_idxs = (-pred).argsort()[0:5]\n","#print(preditem_idxs)\n","\n","# display the recommendations\n","for idx in preditem_idxs:\n","  print(titles[titles.itemidx == idx][[\"movie id\",\"movie name\"]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gyv01pPr-9rj"},"source":["# for comparison, look at the items the target has rated most highly in the past\n","print(\"\\nItems with high ratings from the target user\\n\",\"=\"*40)\n","toprated_itemids = ratings_df[ratings_df.user_id == targetuser].sort_values(by=['rating'], ascending=False).head(5).item_id.values\n","for i in toprated_itemids:\n","  print(titlelookup[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8fYCrSLJYpy"},"source":["ASIDE:  Another way to use the embeddings is to use them to create a recommendation engine based on similar items (like content-based filtering). In this case the embedding's would be used instead of the content features. We would use a similarity measure to finds the nearest items to the items already liked by the user.\n"]},{"cell_type":"markdown","metadata":{"id":"xGq_qi4uObNU"},"source":["# WORKSHOP EXTENSION  (Optional if time)\n","Try alternative model achitectures and see which gives the best MAE on the test data"]},{"cell_type":"code","metadata":{"id":"SJE1vuZbO_HS"},"source":["# E.g. one alternative model is to concatenate the user and item embeddings and use this as input into a 2-layer classification network\n","# To do this insert the following lines into the appropriate place in the definemodel() function above\n","# then repeat the model build and test to see if it gives better results!\n","\n","out = ks.layers.Concatenate()([items_flat, users_flat])\n","out = ks.layers.Dense(128, activation='relu')(out)\n","out = ks.layers.Dense(1, activation='relu')(out)"],"execution_count":null,"outputs":[]}]}