{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13Etf-htPSFmTJl6C3hqEgKDPjRnNxwED","timestamp":1673671733973}],"authorship_tag":"ABX9TyPS+0/JMxldS7NHqiDyJ+aB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BMIrsbcHEItc"},"source":["# This workshop demonstrates how to apply the ALS algorithm within pyspark ml library to the Movielens dataset and also to the Deskdrop dataset.\n"]},{"cell_type":"code","metadata":{"id":"U-nm6f6Qcw36","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857571516,"user_tz":-480,"elapsed":53824,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"6cbc7184-650d-4a69-bd37-bec28e78330c"},"source":["pip install pyspark"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=8c31e0de5dac1d86f9e16352c6086446eb0b2c343c57a9f2926e1a3ad1373089\n","  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}]},{"cell_type":"code","metadata":{"id":"L7EUaWspd-fR","executionInfo":{"status":"ok","timestamp":1673857606665,"user_tz":-480,"elapsed":307,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["from pyspark.sql import Row\n","from pyspark.sql import SparkSession\n","from pyspark.ml.recommendation import ALS\n","from pyspark.ml.evaluation import RegressionEvaluator"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOqQ3uqTeCo5","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1673857617754,"user_tz":-480,"elapsed":8619,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"45136518-6de9-4d7a-b01c-a18a7737c648"},"source":["spark = SparkSession.builder.getOrCreate()\n","#spark = SparkSession.builder.appName('workshop4B').master(\"local[*]\").getOrCreate() # give more details, e.g. if you had 4 cores you could replace * with 4:\n","spark"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f0f0fb07040>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://24cc89a02e25:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WKdjV7E4s2-M"},"source":["# Note: if you want to stop the spark session at any time use the below\n","spark.stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddxRp3RNh1Fx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857676754,"user_tz":-480,"elapsed":21525,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"a6322363-f904-4f1a-f9e1-216274674433"},"source":["# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["We begin by first demonstrating how to use pySpark ALS on explicit ratings data (Movielens)..... "],"metadata":{"id":"ePn6mUA75IcO"}},{"cell_type":"markdown","metadata":{"id":"OT-C0A3vhmvA"},"source":["## Step1: Load and prepare the Movielens data"]},{"cell_type":"code","metadata":{"id":"Py0LHfW2ksV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857754567,"user_tz":-480,"elapsed":7574,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"2ad0b208-4343-41a0-d6d7-ccdb8a6a0f2d"},"source":["# read in the movielens 100K datatset into a pyspark dataframe (a distributed dataframe)\n","# FYI: more info on spark dataframes: https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\n","\n","file = \"/content/drive/My Drive/recsys/u_data.csv\"\n","ratings_spdf = spark.read.csv(file, header=True)\n","newcolnames = ['userid','itemid','rating','datetime']\n","for c,n in zip(ratings_spdf.columns,newcolnames):\n","    ratings_spdf=ratings_spdf.withColumnRenamed(c,n)\n","ratings_spdf.printSchema()\n","ratings_spdf.show(10)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- userid: string (nullable = true)\n"," |-- itemid: string (nullable = true)\n"," |-- rating: string (nullable = true)\n"," |-- datetime: string (nullable = true)\n","\n","+------+------+------+---------+\n","|userid|itemid|rating| datetime|\n","+------+------+------+---------+\n","|     1|     1|     5|874965758|\n","|     1|     2|     3|876893171|\n","|     1|     3|     4|878542960|\n","|     1|     4|     3|876893119|\n","|     1|     5|     3|889751712|\n","|     1|     6|     5|887431973|\n","|     1|     7|     4|875071561|\n","|     1|     8|     1|875072484|\n","|     1|     9|     5|878543541|\n","|     1|    10|     3|875693118|\n","+------+------+------+---------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"S1GzPRWAJpVt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857763138,"user_tz":-480,"elapsed":367,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"03d6a113-b228-459d-c7bc-1d8a40a6f5f4"},"source":["# FYI: show how many partitions the data is spread over\n","# for large datsets you can partition the data across multiple CPU's\n","# e.g. see:  https://sparkbyexamples.com/spark/spark-partitioning-understanding/\n","ratings_spdf.rdd.getNumPartitions()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"yTaodiVP5ioa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857770169,"user_tz":-480,"elapsed":867,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"8c642c3a-d6a2-4568-9c48-d6d5b13a480b"},"source":["# convert rating to float\n","ratings_spdf = ratings_spdf.withColumn(\"rating\", ratings_spdf.rating.cast(\"Float\"))\n","ratings_spdf.printSchema()\n","ratings_spdf.select(\"userid\",\"itemid\",\"rating\").show(5)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- userid: string (nullable = true)\n"," |-- itemid: string (nullable = true)\n"," |-- rating: float (nullable = true)\n"," |-- datetime: string (nullable = true)\n","\n","+------+------+------+\n","|userid|itemid|rating|\n","+------+------+------+\n","|     1|     1|   5.0|\n","|     1|     2|   3.0|\n","|     1|     3|   4.0|\n","|     1|     4|   3.0|\n","|     1|     5|   3.0|\n","+------+------+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"fpO_Z9qIF2Wc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857781672,"user_tz":-480,"elapsed":6781,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"7411d7a9-e55c-4826-cb67-3058b70ef2c4"},"source":["# this generates a nice summary of the dataset\n","ratings_spdf.describe().show()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-----------------+------------------+------------------+-----------------+\n","|summary|           userid|            itemid|            rating|         datetime|\n","+-------+-----------------+------------------+------------------+-----------------+\n","|  count|           100000|            100000|            100000|           100000|\n","|   mean|        462.48475|         425.53013|           3.52986|8.8352885148862E8|\n","| stddev|266.6144201275064|330.79835632558417|1.1256735991443165|5343856.189502888|\n","|    min|                1|                 1|               1.0|        874724710|\n","|    max|               99|               999|               5.0|        893286638|\n","+-------+-----------------+------------------+------------------+-----------------+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"yigXlSDR7Azr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857792104,"user_tz":-480,"elapsed":2685,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"aa1129eb-9296-4d01-bf1a-ec41c333f01c"},"source":["# In general, userid and itemid are alphanumeric strings. Hence we need to convert them to numeric to facilitate matrix indexing.\n","# To do this we create dictionaries to map userids and itemids to integer indices (same as we did in workshop2)\n","# (Note: in this datafile, userid and itemid are integers anyway, so we could instead simply cast them to integer instead of creating a map)\n","import numpy as np\n","userids = np.sort([x.userid for x in ratings_spdf.select(\"userid\").distinct().collect()])\n","userid_encode = {x: i for i, x in enumerate(userids)}\n","itemids = np.sort([x.itemid for x in ratings_spdf.select(\"itemid\").distinct().collect()])\n","itemid_encode = {x: i for i, x in enumerate(itemids)}\n","print(len(userids), len(itemids))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["943 1682\n"]}]},{"cell_type":"code","metadata":{"id":"3wgl8rIs_Uag","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857800666,"user_tz":-480,"elapsed":2572,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"fbc5cf4c-4e07-461d-bf82-ff2a05f8c549"},"source":["# copy the integer indices into the ratings dataframe\n","rdd2=ratings_spdf.rdd.map(lambda x: (userid_encode[x[0]],itemid_encode[x[1]],float(x[2])))\n","ratings_spdf = rdd2.toDF()\n","\n","# reinsert the column names\n","for c,n in zip(ratings_spdf.columns,newcolnames):\n","    ratings_spdf=ratings_spdf.withColumnRenamed(c,n)\n","\n","# show the results\n","ratings_spdf.printSchema()\n","ratings_spdf.show(5)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- userid: long (nullable = true)\n"," |-- itemid: long (nullable = true)\n"," |-- rating: double (nullable = true)\n","\n","+------+------+------+\n","|userid|itemid|rating|\n","+------+------+------+\n","|     0|     0|   5.0|\n","|     0|   794|   3.0|\n","|     0|   905|   4.0|\n","|     0|  1016|   3.0|\n","|     0|  1127|   3.0|\n","+------+------+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"WIOzXfS9iNoI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673857810744,"user_tz":-480,"elapsed":5881,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"5b6a81c5-8418-4839-9da6-3ab4ab0f4406"},"source":["# split data into training and test sets (these are also spark dataframes)\n","(training, test) = ratings_spdf.randomSplit([0.8, 0.2])\n","print(type(training))\n","print(\"trainset=\",training.count(), \"test set=\", test.count())"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.sql.dataframe.DataFrame'>\n","trainset= 80008 test set= 19992\n"]}]},{"cell_type":"markdown","metadata":{"id":"yXuv3qP6g3n_"},"source":["## Step2: Build the recommendation model using ALS on the training data\n","\n","Parameters for the ml ALS algorithm:\n","\n","*   numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n","*   rank is the number of latent factors in the model (defaults to 10).\n","*   maxIter is the maximum number of iterations to run (defaults to 10).\n","*   regParam specifies the regularization parameter in ALS (defaults to 1.0).\n","*   implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback). When selecting implicit, the algorithm defined in \"Collaborative Filtering for Implicit datasets (Yifan Hu; Yehuda Koren; Chris Volinsky) is used.\n","*  alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n","*   nonnegative specifies whether or not to use nonnegative constraints for least squares (defaults to false).\n","\n","https://spark.apache.org/docs/latest/ml-collaborative-filtering.html"]},{"cell_type":"code","metadata":{"id":"MfmfsucelYmH","executionInfo":{"status":"ok","timestamp":1673857825979,"user_tz":-480,"elapsed":343,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}}},"source":["# Note: we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n","# By default, Spark assigns NaN predictions during ALSModel.transform when a user and/or item factor is not present in the model. \n","# This is useful in a production system but undesirable during cross-validation, since NaN predicted values will result in NaN results for the evaluation metric.\n","# The number of latent features (rank) is set below at 15. Please experiment with diffent values to try to get optimum values\n","\n","als = ALS(maxIter=20, rank=15, regParam=0.01, userCol=\"userid\", itemCol=\"itemid\", ratingCol=\"rating\", coldStartStrategy=\"drop\", implicitPrefs=False)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# fit the model\n","model = als.fit(training)"],"metadata":{"id":"MQ7GyacGjGaI","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1673857860301,"user_tz":-480,"elapsed":21190,"user":{"displayName":"Barry Shepherd","userId":"14632529924177760561"}},"outputId":"e5732dd2-93a3-4481-d410-70ddd1c75a35"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["----------------------------------------\n","Exception happened during processing of request from ('127.0.0.1', 49596)\n","ERROR:root:Exception while sending command.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-13-e97d9f887b1c>\", line 2, in <module>\n","    model = als.fit(training)\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py\", line 205, in fit\n","    return self._fit(dataset)\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py\", line 383, in _fit\n","    java_model = self._fit_java(dataset)\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py\", line 380, in _fit_java\n","    return self._java_obj.fit(dataset._jdf)\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1321, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\", line 190, in deco\n","    return f(*a, **kw)\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\", line 516, in send_command\n","    raise Py4JNetworkError(\"Answer from Java side is empty\")\n","py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\", line 539, in send_command\n","    raise Py4JNetworkError(\n","py4j.protocol.Py4JNetworkError: Error while sending or receiving\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n","    self.process_request(request, client_address)\n","  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n","    self.finish_request(request, client_address)\n","  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n","    self.RequestHandlerClass(request, client_address, self)\n","  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n","    self.handle()\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 281, in handle\n","    poll(accum_updates)\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 253, in poll\n","    if func():\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 257, in accum_updates\n","    num_updates = read_int(self.rfile)\n","  File \"/usr/local/lib/python3.8/dist-packages/pyspark/serializers.py\", line 595, in read_int\n","    raise EOFError\n","EOFError\n","----------------------------------------\n"]},{"output_type":"error","ename":"ConnectionRefusedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3326\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3327\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-e97d9f887b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3341\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3342\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3345\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;34m'traceback'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;34m'ename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;34m'evalue'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     }\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipython_genutils/py3compat.py\u001b[0m in \u001b[0;36msafe_unicode\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# Note: technically this should return a bytestring 'str' rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"]}]},{"cell_type":"code","metadata":{"id":"-ykwjDn74ESD"},"source":["# Evaluate the model by computing the MAE (or RMSE) on the test data\n","# (try comparing the performance treating the data as explicit ratings versus implicit ratings)\n","predictions = model.transform(test)\n","evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n","error = evaluator.evaluate(predictions)\n","print(\"Mean Absolute error = \", error)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJdmQv1nIwQl"},"source":["# Generate top 10 movie recommendations for each user\n","userRecs = model.recommendForAllUsers(10)\n","# Generate top 10 recommendations for each movie (ie for each movie show the top 10 'similar' movies (Note: similarity is based on user ratings not item features)\n","movieRecs = model.recommendForAllItems(10)\n","\n","# Note: both of the above outputs are Spark dataframes\n","userRecs.show(10)\n","movieRecs.show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate top 10 movie recommendations for each user\n","userRecs = model.recommendForAllUsers(10)"],"metadata":{"id":"LCL-qAhjrUbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8lBRnrWkPe6Y"},"source":["# To view the recommendations using the actual movie names we first load the movie names from file.\n","import pandas as pd\n","file = \"/content/drive/My Drive/recsys/u_item.csv\"\n","itemdata = pd.read_csv(file, dtype=str)\n","print(itemdata[0:5]) # show a sample of the file, it also contains movie genre info - but we ignore this\n","titlelookup = dict(zip(itemdata[\"movie id\"],itemdata[\"movie name\"])) # create a lookup dictionary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzz593XUgWkO"},"source":["# now swap the actual movie names into the movie->movie recommendations\n","recs = movieRecs.head(5)\n","for i in recs:\n","  print(\"target content=\",titlelookup[itemids[i[0]]])\n","  print(\"recommended content:\")\n","  for rec in i[1]:\n","    print(titlelookup[itemids[rec[0]]])\n","  print(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tt1zudHdWnqu"},"source":["Note: example code for using MLlib ALS is shown below"]},{"cell_type":"code","metadata":{"id":"-Oplo6IHU4no"},"source":["# FYI ASIDE: an example of using MLlib ALS\n","# note that model.train and model.trainImplicit are part of MLlib and operate on rdd data (not dataframe)\n","# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.recommendation.ALS.html\n","\n","import pyspark.mllib.recommendation as mllr\n","als = mllr.ALS()\n","mll_model = als.train(training.rdd, rank=15, iterations=20, lambda_=0.01)\n","#mll_model = als.trainImplicit(training.rdd, rank=15, iterations=20, lambda_=0.01) # use if the ratings are implicit"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fAVlCPvO4H7E"},"source":["# Now we load the Deskdrop dataset, create an integer implicit rating and then compare the performance of implicit versus explict ALS on the same dataset\n","\n","In this section, we show how a variety of implicit signals can be assembled into an integer (non-binary) implicit ratings matrix. This is then factorised using the Spark implicit ALS algorithm and used to generate recommendations.\n","\n","The data set used is the deskdrop dataset. Deskdrop is an internal communications platform that allows company employees to share relevant articles with their peers, and collaborate around them. \n"]},{"cell_type":"markdown","metadata":{"id":"ChMNSeSKF05h"},"source":["**part1: In the first half of this section we use Pandas to preprocess the data**"]},{"cell_type":"code","metadata":{"id":"7_uAi8RGl3y5"},"source":["import pandas as pd\n","# import pyspark.pandas as ps  # alternative method"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NetU0b51EgR7"},"source":["# load the deskdrop interaction events (12 months worth) for individual users\n","# the type of interaction (view, like, bookmark, follow, comment) is stored in the variable eventType\n","file = \"/content/drive/My Drive/recsys/deskdrop_users_interactions.csv\"\n","\n","interactions_df = pd.read_csv(file)\n","interactions_df.drop(['userAgent', 'userRegion', 'userCountry'], axis=1, inplace=True)\n","interactions_df.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bs5Uf7wtOJRS"},"source":["#view the frequency of the different types of events\n","ratings_df = interactions_df[['contentId','personId', 'eventType']]\n","ratings_df['eventType'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BalpHtA2Eitc"},"source":["# Create an implict integer rating called eventStrength based on the type of the interaction with the article\n","# E.g, assume a bookmark indicates a higher interest than a like etc.\n","# To do this, create a dictionary to associate each eventType with a weight.\n","event_type_strength = {\n","   'VIEW': 1.0,\n","   'LIKE': 2.0, \n","   'BOOKMARK': 3.0, \n","   'FOLLOW': 4.0,\n","   'COMMENT CREATED': 5.0,  \n","}\n","#ratings_df['eventStrength'] = ratings_df['eventType'].apply(lambda x: event_type_strength[x]) # using direct assignment gives a warning\n","temp = ratings_df['eventType'].apply(lambda x: event_type_strength[x])\n","ratings_df.insert(3,\"eventStrength\",temp,True)\n","ratings_df.sample(5) # show a sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcy9gM3kQPlK"},"source":["# Optional: \n","# ignore repeat interactions by a user on a document that have the same type (eg ignore multiple views of a document)\n","# This is debatable: repeat views may indicate more interest in a document so perhaps we should keep, but do repeat bookmarks mean more interest or just forgetfulness?\n","# One variant is to ignore repeated likes, bookmarks and follows but do not ignore repeated views?\n","# If you have time, try experimenting with your own mapping of user interactions to implicit user ratings\n","print(ratings_df.shape) # show size before\n","ratings_df = ratings_df.drop_duplicates() \n","print(ratings_df.shape) # show size after"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9nUm907OaFS"},"source":["# if a user has multiple interactions on the same content then sum the event strengths\n","# (we now use the summed eventStrengths as the implicit integer ratings)\n","ratings_df = ratings_df.groupby(['personId', 'contentId']).sum().reset_index()\n","print(ratings_df.shape)\n","ratings_df.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4TMhnqcF_zD"},"source":["# plot the eventStrengths for each user/content\n","import matplotlib.pyplot as plt\n","plt.plot(ratings_df.personId, ratings_df.eventStrength.sort_values())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6868QLWa8XYY"},"source":["# alternatively, we can plot the eventStrengths as a histogram\n","plt.hist(ratings_df.eventStrength, bins = 50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Y7J9bJNAgrB"},"source":["# optional: truncate the strengths at 20 \n","ratings_df.loc[ratings_df.eventStrength > 20, 'eventStrength'] = 20\n","ratings_df.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQZKnjBO-DtT"},"source":["# convert column names to standard names\n","ratings_df.columns = ['userid','itemid','rating']\n","ratings_df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Otfn0s35D8ah"},"source":["# convert user and item IDs to integers\n","ratings_df = ratings_df.astype({\"userid\": int, \"itemid\":int})\n","ratings_df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLy4XrZoH6E1"},"source":["**part2:  now we convert the generated ratings to a spark dataframe and then build and apply the ALS model as was done above with Movielens**\n","\n","Compare results obtained using explict ALS with those from implicit ALS, does treating the ratings as implicit give better results?"]},{"cell_type":"code","metadata":{"id":"3rTc8EyqyUmx"},"source":["# convert from pandas dataframe to spark dataframe\n","ratings_spdf = spark.createDataFrame(ratings_df)\n","ratings_spdf.printSchema()\n","ratings_spdf.show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTKwb6sF4DgQ"},"source":["# We convert the userID and itemID  to numeric indexes to facilitate matrix indexing.\n","# first create the mappings as dictionaries\n","import numpy as np\n","userids = np.sort([x.userid for x in ratings_spdf.select(\"userid\").distinct().collect()])\n","userid_encode = {x: i for i, x in enumerate(userids)}\n","itemids = np.sort([x.itemid for x in ratings_spdf.select(\"itemid\").distinct().collect()])\n","itemid_encode = {x: i for i, x in enumerate(itemids)}\n","print(len(userids), len(itemids))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RlBBtHa188NR"},"source":["# now copy the integer indices into the ratings dataframe\n","rdd2=ratings_spdf.rdd.map(lambda x: (userid_encode[x[0]],itemid_encode[x[1]],float(x[2])))\n","ratings_spdf = rdd2.toDF()\n","# restore the column names\n","newcolnames = ['userid','itemid','rating']\n","for c,n in zip(ratings_spdf.columns,newcolnames):\n","    ratings_spdf=ratings_spdf.withColumnRenamed(c,n)\n","ratings_spdf.show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojOsguc02hcN"},"source":["# split data into training and test sets\n","(training, test) = ratings_spdf.randomSplit([0.8, 0.2])\n","print(\"trainset=\",training.count(), \"test set=\", test.count())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8Yol0Mn2Yba"},"source":["# perform the matrix factorisation\n","als = ALS(maxIter=20, rank=15, regParam=0.01, userCol=\"userid\", itemCol=\"itemid\", ratingCol=\"rating\", coldStartStrategy=\"drop\", implicitPrefs=True)\n","model = als.fit(training)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImZoWy0qJxV0"},"source":["# Evaluate the model by computing the MAE (or RMSE) on the test data\n","predictions = model.transform(test)\n","evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n","error = evaluator.evaluate(predictions)\n","print(\"Mean Absolute error = \", error)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"up_xs-dYDuo7"},"source":["# is the above MAE good or bad? calculating the stdev of the rating may help decide\n","from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n","\n","stats = training.select(\n","    _mean(col('rating')).alias('mean'),\n","    _stddev(col('rating')).alias('std')\n",").collect()\n","\n","mean = stats[0]['mean']\n","std = stats[0]['std']\n","print(mean,std)"],"execution_count":null,"outputs":[]}]}